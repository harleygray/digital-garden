{"/":{"title":"üì° hi, welcome","content":"# hi, welcome\nthanks for stopping by! this is my digital garden; a space to share my musings. like any garden, there are parts that are fully matured, some still growing, and others that have only just sprouted. \n\n# a note from your tour guide\nthis website allows you to visualise how my notes are connected with each other, and encourages you to read in a non-linear manner. the 'interactive graph' at the bottom of the page will show you what thoughts live in the neighbourhood of this one.\n\ni will link to other notes related to the one you're currently on. to save you time, i'll mostly link to fully developed ideas, but you can find notes that are still 'growing' if you care to.  \n\nmy strategy to craft a meaningful life can be found [[notes/blueprint map|here]]\n\nif you're into machine learning, this [[notes/catch these hands, please|ai can tell what hand gesture you're making]]\n\nlfg!!\n![[notes/images/in and out, quick adventure.jpg]]","lastmodified":"2022-10-15T04:20:21.997225538Z","tags":null},"/notes/blueprint-map":{"title":"blueprint","content":"# what is this for?\ncrafting a life full of meaning. this is my ambitious north star, and though i can't see the full picture of this, i *can* see the direction. by focusing on the tools, technology and people that resonate with me, i hope to aim at my higher ideal with ever increasing clarity. \n\ni imagine my curious spirit as a light that guides my attention. up until recently it had been wandering off on its own, leaving me feeling lost. reconnecting with that light has been my quest, with the hopeful reward of being guided back to a path of excitement and purpose.\n\nit's difficult to find meaning in life without anything guiding us. we may start to indulge in things that feel good in the moment, but those eventually lose their lustre. we need something to strive for, something that inspires us to take action each day\n\n![baroque oil painting of anime key visual concept art of touhou anime witches flying on broomsticks through the sky, volumetric lighting, sunrays breaking through clouds, grimdark steampunk high fantasy, trending on artstation, brush strokes, oil on canvas, style of makoto shinkai and greg rutkowski and studio ghibli ](https://image.lexica.art/md/800b6b52-32ab-4dc2-b85b-126427f3a9df)\n\n\n# what is a blueprint?\na blueprint is an action plan. it's a focal point for people (or one person collaborating with themselves) to concentrate their energy. large projects require coordination, therefore creating a plan--even one that will be updated--makes success more likely.\n\nwhat is success? what am i creating a blueprint *of*? if everything went perfectly, i'd be:\n1. contributing to a cause larger than myself\n2. building in public - website, tweets, applets\n3. learning about interesting technology through experimentation\n4. helping my community with remaining resources\n\nnumber 1. is the north star, but it is the point that i've been struggling with for months now. it's becoming clearer, but in trying too hard to find it i've become stuck. by contrast, 2, 3 and 4 are within my grasp *now*. by starting with these three, i hope that 1. becomes more clear\n\n![a vertical flowchart explaining life and death, tree of life, buddhism, ancient, fractals, beautiful lighting, octane rendered, high detail, photograph of the year, trending on art station and beyond ](https://image.lexica.art/md/3ddb7700-3583-428b-b2c3-1c4196c6c686)\n\n# how to progress\n## contribute\ni am searching for narrative and digital technology to help us solve the immense challenges we face. we have the tools, but we lack a story that unites us. \n\ntechnologies that i believe are part of a solution are:\n- [holochain](https://www.holochain.org/), an end-to-end open source P2P app framework \n- language models, particularly implementations such as [Irises](https://twitter.com/CognicistIris)\n- decentralised protocols generally\n\neach of these technologies are in their infancy. their development will influence, and be influenced by, the desires of individuals as well as the global zeitgeist. i'm optimistic about the potential of these technologies and want to be a part of a team using them to help people work together\n\n## build\n### writing and tweeting\nthis blog is one step in the direction of [[notes/unstucking_myself|\"UnStucking\" myself]]. i have been stuck for a while, caught by my insistence on things being perfect before i release them into the world. this has reached a stage of being unhelpful (mostly) and i want to break the cycle. also, i've been making a concerted effort into tweeting more. part of feeling Stuck was imagining that i have nothing to say...which isn't true. \n\ni write a lot. like, almost every day. writing my thoughts down is illuminating, but i've built up a mindset causing me to isolate these thoughts. though some thoughts are obviously best kept to myself, i'm out of balance. this website is me making strides back to the direction of balance.\n\n### tech projects\nas part of [fast.ai's deep learning course](https://course.fast.ai/), i'm building and sharing machine-learning based applets. currently in progress is a hand-gesture recognition model that will help me document my mindset moving forward. more on in this space will be linked to [[notes/data science map|here]]\n\n## learn\nrecently i've had an 'aha!' moment: that learning to harness the power of machine learning is something that *feels* right. but there is so much to learn. not only about how this rapidly advancing technology works, but how the mis-use of it can cause harm. surely when Prometheus stole fire, a few villages burned themselves to the ground. minimising the harm of this technology needs to be taken into account as we wield the power of the gods.\n\nlearning and building go hand-in-hand, and so i believe that working through [fast.ai's deep learning course](https://course.fast.ai/) and documenting my progress will move me forward in this direction. \n\n![thoth the egyptian god floating beautifully, fantasy, dramatic, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by Gustave Dore, octane render](https://image.lexica.art/md/691f7661-7240-4c2f-9728-9f71a6dd1528)\n\n## help\nfinally, but no less importantly, i want to lift others up. a rising tide lifts all ships. i don't have a grand plan on how to do this at a large scale, but i do know things that work on a small one. \n\ni've joined my local emergency services in response to increasing natural disasters occuring world-wide. though i haven't responded to a catastrophic event yet, i am training to do so because i know it's coming. training is a weekly endeavour (which is exhausting in its own right) that i've commited to already and will likely write about here in due course.\n\non a personal level, i strive to listen always and offer help when it's needed. i don't always get this right, nor do i always have the energy to do so. i have a habit of giving too much of myself when i'm stretched thin, which isn't good for anyone. part of helping others means putting the oxygen mask on myself first. this way i can have more of myself to give","lastmodified":"2022-10-15T04:20:21.997225538Z","tags":null},"/notes/catch-these-hands-please":{"title":"üñêüèΩcatch these hands, pleaseüñêüèΩ","content":"# surprised by simplicity\nwhat is the hardest part of developing machine learning models? as a budding practitioner i assumed it would be the coding or the maths, but i underestimated a key aspect: finding data. this is a surprisingly demanding task and is essential in order to build useful models.  \n\nmy goal here is to point beginners in a direction that minimises frustration and maximises cool things built. \n\nthe basic flow of this project (and blog post) is as follows:\n1. define the task\n2. acquire/clean data\n3. train the model\n\ntask: train a model that can recognise hand gestures in photos. i want it to recognise at least üëçüèΩ and üëéüèΩ, and optimally one-five fingers up too.\n\nto get this project going quickly, we can start with a 'pre-trained' model that has been developed by experts and trained on large databases. rather than building a model from scratch, we can stand on the shoulders of giants and modify a pre-trained model for this purpose.  \n\nthe crux of this project boils down to this step: modifying (or *fine tuning*) a pre-trained model. the starting model is called resnet18, which was trained on more than a million images. it's very efficient at image classification and so is perfect to be fine-tuned. because resnet18 is already able to recognise objects, the fune-tuning step requires us to provide specific data on the thing we want to recognise. in this case, we'll need to provide images of hand gestures and the corresponding 'label', i.e. what hand gesture is being done in a photo. \n\n\n# the search for data\nso the search for data began. i tried DuckDuckGo, Google, and even Bing! i tried many combinations of search terms, such as  `hand thumb down -angry` but to no avail. i considered supplying the model with my own photos, but realised that the amount i would need to do would be impractical. then, unexpectedly, Hagrid saved me.\n\n![[notes/images/hagrid.jpg]]\n\ni mean, [HaGRID - HAnd Gesture Recognition Image Dataset](https://arxiv.org/abs/2206.08219). this is an enormous dataset of ~550k images, each of a person doing one of 18 gestures such as thumbs up, one finger, two fingers, etc. wonderful! just what i was looking for. \n\n\u003e [!hint] where to find data\n\u003e\n\u003e Kaggle has all kinds of cleaned data. You'll find data suitable for similar tasks [here](https://www.kaggle.com/datasets?tags=13207-Computer+Vision)\n\nthe full size of this dataset is 716GB, the largest dataset i've ever worked with. i figured that i wouldn't be able to just use my free tier of Kaggle to handle such a large dataset, so i used the supplied sample dataset which containsed 100 of each of the 18 gestures. training a model on this set was a good first step, but didn't provide the results to be useful.\n\nnext, i found a scaled down version of this dataset, with 512p images instead of the higher quality original photos. these were modified to be lower resolution for image classification instead of object detection (i.e. recognising where in the image a hand is located). i hadn't even realised that the original HaGRID dataset was meant for both purposes. by finding this dataset, i was able to continue with the project. \n\n\n\n\n# model\n\u003e after loading the dataset, it was time to get to grips with the fast.ai library. there were many notebooks online from which to draw inspiration and lessons. in particular, there was a notebook which went through the different steps of getting the image files from the kaggle dataset, marking them with a classification label so it could then be loaded into the fast.ai library. i still have lots to learn here, but i accept glossing over this particular section in the name of learning more deep learning.\n\nafter loading the dataset, it was time to grapple with the fast.ai library. luckily there are many notebooks online from which to draw inspiration and lessons. in particular [this](https://www.kaggle.com/code/stpeteishii/hagrid-18-classify-fasiai/data) notebook went through the different steps of accessing the files from the kaggle dataset, marking them with a classification label so they can be loaded into the fast.ai library. \n\nonce data is prepared, it needs to be packaged into a \"data loader\" object, `dls` in the code block below. here, the images are already in the `train_df` dataframe, with `fn_col` and `label_col` instructing the program where to look for paths to the image and its label, respectively. the `valid_pct` parameter is the percentage of items to set aside for verification in each training epoch.\n\n```python\ndls = ImageDataLoaders.from_df(train_df, \n                               fn_col=0, #path\n                               label_col=1, #label\n                               folder='./', \n                               valid_pct=0.2, \n                               item_tfms=Resize(224))\ndls.show_batch(max_n=9)\n```\n\n![](https://www.kaggleusercontent.com/kf/107717335/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Gm8Fz8Ptp1uHi8G9I9iaQQ.K4SK7TDyxs6a4pvyK_B7ATHsGQ8qzuBxaQnK-fA9y4pI8b8ffdPRnV2RBFvaqMEOJxsVjreD-EKHz-5d7FSOBeXtXxKx_T9eiYJzaNHDf0lesuj0qa2MaTs427L__gQ6MGyiBnQ34HCfsXD71m4v2KDAa-l-S_OnB4zoFRwakD80162t8OlYQKNtmriVzjX1ThJmumkc3LHOtfnqmmRwp3oq1P-TcICjsOLVRRuwd0qcIJmcs_sVxLWME3Mu556OdwkAqL9G2oO0DULip2X2cKffhauxA4jChxPIvRbj6RQYpM_cC9krZu_SyzgG9lXfy_BU9Jq7mTQ94gbKVSBgURBoRxm8qjiDtbqHqazkZ34DiMZ6gHie1WZB1KTS9jSdzmjMs0KShX3jEaIEsYOuhoxHrLjllALZiNGkr6CdUj4xBNKxQosMIFdBdGiljQK7HwF4kXFjp4xa63qCmWn0ViB29GM3aJix79SSK17JktpArENhqjhmlhB_LZ1fN33Df8wJkDPOIeO-LwltO8Vfia593oP8oFUAaZ_gyU58NR4sQNFPp7IA447FH__nO1mqVZ4SbW2T7CKjkur025eMmRPI8RLJOFssGj15M64syV8k73TFPg6NBs-zkpzm7B40brm9QaGRcPmCFnCDsQ8hdGnYKiQuL7IUk0HVKqPXyvI.Jux3vjQ24F75ZM-NquPT_A/__results___files/__results___11_0.png)\n\neach of the nine images above are now the same size, and have been labelled appropriately. after the images are loaded in and labelled, the next step is to pass them into the deep learner. here is where we specify the pre-trained model (resnet18) that we'd like to begin with.  \n\n```python\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(12)\n```\n\n![[notes/images/fine_tuning.png]]\n\nafter twelve cycles, the model correctly classifies hand gestures 70% of the time. any further fine tuning did not make this any more accurate. i have some ideas as to why that might be, which i will investigate in the future. \n\nthe model has been deployed to a site called HuggingFace [here](https://huggingface.co/spaces/harleygray/fastai-hand-classifier). you can upload your own photo of you doing a hand gesture and see if it recognises which you are doing! ","lastmodified":"2022-10-15T04:20:21.997225538Z","tags":null},"/notes/config":{"title":"Configuration","content":"\n## Configuration\nQuartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you'd like to get.\n\nThe majority of configuration can be found under `data/config.yaml`. An annotated example configuration is shown below.\n\n```yaml {title=\"data/config.yaml\"}\n# The name to display in the footer\nname: Jacky Zhao\n\n# whether to globally show the table of contents on each page\n# this can be turned off on a per-page basis by adding this to the\n# front-matter of that note\nenableToc: true\n\n# whether to by-default open or close the table of contents on each page\nopenToc: false\n\n# whether to display on-hover link preview cards\nenableLinkPreview: true\n\n# whether to render titles for code blocks\nenableCodeBlockTitle: true \n\n# whether to render copy buttons for code blocks\nenableCodeBlockCopy: true \n\n# whether to render callouts\nenableCallouts: true\n\n# whether to try to process Latex\nenableLatex: true\n\n# whether to enable single-page-app style rendering\n# this prevents flashes of unstyled content and improves\n# smoothness of Quartz. More info in issue #109 on GitHub\nenableSPA: true\n\n# whether to render a footer\nenableFooter: true\n\n# whether backlinks of pages should show the context in which\n# they were mentioned\nenableContextualBacklinks: true\n\n# whether to show a section of recent notes on the home page\nenableRecentNotes: false\n\n# whether to display an 'edit' button next to the last edited field\n# that links to github\nenableGitHubEdit: true\nGitHubLink: https://github.com/jackyzha0/quartz/tree/hugo/content\n\n# whether to use Operand to power semantic search\n# IMPORTANT: replace this API key with your own if you plan on using\n# Operand search!\nenableSemanticSearch: false\noperandApiKey: \"REPLACE-WITH-YOUR-OPERAND-API-KEY\"\n\n# page description used for SEO\ndescription:\n  Host your second brain and digital garden for free. Quartz features extremely fast full-text search,\n  Wikilink support, backlinks, local graph, tags, and link previews.\n\n# title of the home page (also for SEO)\npage_title:\n  \"ü™¥ Quartz 3.3\"\n\n# links to show in the footer\nlinks:\n  - link_name: Twitter\n    link: https://twitter.com/_jzhao\n  - link_name: Github\n    link: https://github.com/jackyzha0\n```\n\n### Code Block Titles\nTo add code block titles with Quartz:\n\n1. Ensure that code block titles are enabled in Quartz's configuration:\n\n    ```yaml {title=\"data/config.yaml\", linenos=false}\n    enableCodeBlockTitle: true\n    ```\n\n2. Add the `title` attribute to the desired [code block\n   fence](https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences):\n\n      ```markdown {linenos=false}\n       ```yaml {title=\"data/config.yaml\"}\n       enableCodeBlockTitle: true  # example from step 1\n       ```\n      ```\n\n**Note** that if `{title=\u003cmy-title\u003e}` is included, and code block titles are not\nenabled, no errors will occur, and the title attribute will be ignored.\n\n### HTML Favicons\nIf you would like to customize the favicons of your Quartz-based website, you \ncan add them to the `data/config.yaml` file. The **default** without any set \n`favicon` key is:\n\n```html {title=\"layouts/partials/head.html\", linenostart=15}\n\u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n```\n\nThe default can be overridden by defining a value to the `favicon` key in your \n`data/config.yaml` file. For example, here is a `List[Dictionary]` example format, which is\nequivalent to the default:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon:\n  - { rel: \"shortcut icon\", href: \"icon.png\", type: \"image/png\" }\n#  - { ... } # Repeat for each additional favicon you want to add\n```\n\nIn this format, the keys are identical to their HTML representations.\n\nIf you plan to add multiple favicons generated by a website (see list below), it\nmay be easier to define it as HTML. Here is an example which appends the \n**Apple touch icon** to Quartz's default favicon:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon: |\n  \u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n  \u003clink rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"\u003e\n```\n\nThis second favicon will now be used as a web page icon when someone adds your \nwebpage to the home screen of their Apple device. If you are interested in more \ninformation about the current and past standards of favicons, you can read \n[this article](https://www.emergeinteractive.com/insights/detail/the-essentials-of-favicons/).\n\n**Note** that all generated favicon paths, defined by the `href` \nattribute, are relative to the `static/` directory.\n\n### Graph View\nTo customize the Interactive Graph view, you can poke around `data/graphConfig.yaml`.\n\n```yaml {title=\"data/graphConfig.yaml\"}\n# if true, a Global Graph will be shown on home page with full width, no backlink.\n# A different set of Local Graphs will be shown on sub pages.\n# if false, Local Graph will be default on every page as usual\nenableGlobalGraph: false\n\n### Local Graph ###\nlocalGraph:\n    # whether automatically generate a legend\n    enableLegend: false\n    \n    # whether to allow dragging nodes in the graph\n    enableDrag: true\n    \n    # whether to allow zooming and panning the graph\n    enableZoom: true\n    \n    # how many neighbours of the current node to show (-1 is all nodes)\n    depth: 1\n    \n    # initial zoom factor of the graph\n    scale: 1.2\n    \n    # how strongly nodes should repel each other\n    repelForce: 2\n\n    # how strongly should nodes be attracted to the center of gravity\n    centerForce: 1\n\n    # what the default link length should be\n    linkDistance: 1\n    \n    # how big the node labels should be\n    fontSize: 0.6\n    \n    # scale at which to start fading the labes on nodes\n    opacityScale: 3\n\n### Global Graph ###\nglobalGraph:\n\t# same settings as above\n\n### For all graphs ###\n# colour specific nodes path off of their path\npaths:\n  - /moc: \"#4388cc\"\n```\n\n\n## Styling\nWant to go even more in-depth? You can add custom CSS styling and change existing colours through editing `assets/styles/custom.scss`. If you'd like to target specific parts of the site, you can add ids and classes to the HTML partials in `/layouts/partials`. \n\n### Partials\nPartials are what dictate what gets rendered to the page. Want to change how pages are styled and structured? You can edit the appropriate layout in `/layouts`.\n\nFor example, the structure of the home page can be edited through `/layouts/index.html`. To customize the footer, you can edit `/layouts/partials/footer.html`\n\nMore info about partials on [Hugo's website.](https://gohugo.io/templates/partials/)\n\nStill having problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n\n## Language Support\n[CJK + Latex Support (ÊµãËØï)](notes/CJK%20+%20Latex%20Support%20(ÊµãËØï).md) comes out of the box with Quartz.\n\nWant to support languages that read from right-to-left (like Arabic)? Hugo (and by proxy, Quartz) supports this natively.\n\nFollow the steps [Hugo provides here](https://gohugo.io/content-management/multilingual/#configure-languages) and modify your `config.toml`\n\nFor example:\n\n```toml\ndefaultContentLanguage = 'ar'\n[languages]\n  [languages.ar]\n    languagedirection = 'rtl'\n    title = 'ŸÖÿØŸàŸÜÿ™Ÿä'\n    weight = 1\n```\n","lastmodified":"2022-10-15T04:20:21.997225538Z","tags":null},"/notes/data-science-map":{"title":"data science map","content":"\nempty, for now!","lastmodified":"2022-10-15T04:20:21.997225538Z","tags":null},"/notes/machine-learning-map":{"title":"machine learning map","content":"\n\n![[notes/catch these hands, please#project statement]]","lastmodified":"2022-10-15T04:20:22.005225492Z","tags":null},"/notes/map-of-maps":{"title":"map of maps","content":"\nmaps are invaluable assets for adventurers. think of this page as a 'central hub', from which you can find entry points into areas of interest. \n\n![[notes/images/Lumbridge_map.png]]\n\n# maps, tags and searches, oh my!\nthis site is intentionally designed to be enjoyable to navigate, and so there are a few options for finding a note that interests you. \n\nfirstly you have a map--like this page--that houses links to other pages with *context* provided. that is, i've provided a summary of linked notes however feels most natural. \n\nif however you'd prefer to simply see all notes tagged in a certain way, the tags page is for you. you can browse through a growing list of notes related to that tag and take it from there. i've even made a spicy move to tag all my maps with the #map tag, so you can view a list of all maps [here](/tags/Map) \n\nthe final option is the search bar in the top-right of every page. click this or use `Ctrl + K` to *quickly* find a phrase or word anywhere within my writing\n\n# blueprint\na blueprint is a model or prototype. this idea is helpful for me as i build a life full of meaning. from where i'm standing, there are ideas like shining lights beckoning me towards them. to intentionally move in the direction of purpose, i've built a work-in-progress plan of action--a [[notes/blueprint map]]--that can guide me through the tumultuous road ahead\n\nfind all notes with the blueprint tag [here](/tags/Blueprint)\n\n# data science / machine learning\ndata can be beautiful or boring. it can inspire insight or elicit confusion. handling data is the realm that i get to explore my creativity, curiosity and technical ability. there's an endless trove of applications and concepts to be documented, which is where this website comes in.  \n\n\u003e [!quote] why i'm blogging\n\u003e\n\u003e You are best positioned to help people one step behind you.\n\u003e \n\u003e -[ Rachel Thomas](https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045)\n\nin the timeless lyrics of Phil Collins: \"in learning you will teach, and in teaching you will learn\". the abundance of free, powerful tools and easily accessible data means that anybody can be a scientist. permission from an academic institution is *not* required. all that you need is the know-how, and in building up my own i hope to teach you something along the way.\n\ndata science and machine learning often go hand in hand, but not always. as such they have distinct maps and tags:\n- machine learning tags are [here](/tags/ML), and the map [[notes/machine learning map|here|]] \n- for data science, [here](/tags/datascience) is the tagged notes, and the map [[notes/data science map|here]] \n\n\n","lastmodified":"2022-10-15T04:20:22.005225492Z","tags":null},"/notes/unstucking_myself":{"title":"UnStucking Myself","content":"","lastmodified":"2022-10-15T04:20:22.005225492Z","tags":null}}