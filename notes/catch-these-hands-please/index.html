<!doctype html><html lang=en><head><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://harleygray.github.io/digital-garden/notes/images/TwitterCard.png"><meta name=twitter:title content="üñêüèΩcatch these hands, pleaseüñêüèΩ"><meta name=twitter:description content="training a neural network to recognise hand gestures"><meta charset=utf-8><meta name=description content="surprised by simplicity what is the hardest part of developing machine learning models? as a budding practitioner i assumed it would be the coding or the maths, but i underestimated a key aspect: finding data."><title>üñêüèΩcatch these hands, pleaseüñêüèΩ</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://harleygray.github.io/digital-garden//icon.png><link href=https://harleygray.github.io/digital-garden/styles.7153093e4d1bbb584a28469cadfa3f88.min.css rel=stylesheet><link href=https://harleygray.github.io/digital-garden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://harleygray.github.io/digital-garden/js/darkmode.3e60c19c41a168209c19407ba798ed3f.min.js></script>
<script src=https://harleygray.github.io/digital-garden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://harleygray.github.io/digital-garden/js/popover.53ad9a087e3feeaaa12b63bfd02d923b.min.js></script>
<script src=https://harleygray.github.io/digital-garden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://harleygray.github.io/digital-garden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://harleygray.github.io/digital-garden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://harleygray.github.io/digital-garden/",fetchData=Promise.all([fetch("https://harleygray.github.io/digital-garden/indices/linkIndex.741f662629cf3ba1ce5b9c7f3421a220.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://harleygray.github.io/digital-garden/indices/contentIndex.fd6b13f5ee2bd6b8110975814c6961f6.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://harleygray.github.io/digital-garden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://harleygray.github.io/digital-garden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'‚Äô':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/harleygray.github.io\/digital-garden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://harleygray.github.io/digital-garden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://harleygray.github.io/digital-garden/>harley's digital garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>üñêüèΩcatch these hands, pleaseüñêüèΩ</h1><p class=meta>Last updated
Oct 15, 2022</p><ul class=tags><li><a href=https://harleygray.github.io/digital-garden/tags/ML/>Ml</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents></nav></details></aside><a href=#surprised-by-simplicity><h1 id=surprised-by-simplicity><span class=hanchor arialabel=Anchor># </span>surprised by simplicity</h1></a><p>what is the hardest part of developing machine learning models? as a budding practitioner i assumed it would be the coding or the maths, but i underestimated a key aspect: finding data. this is a surprisingly demanding task and is essential in order to build useful models.</p><p>my goal here is to point beginners in a direction that minimises frustration and maximises cool things built.</p><p>the basic flow of this project (and blog post) is as follows:</p><ol><li>define the task</li><li>acquire/clean data</li><li>train the model</li></ol><p>task: train a model that can recognise hand gestures in photos. i want it to recognise at least üëçüèΩ and üëéüèΩ, and optimally one-five fingers up too.</p><p>to get this project going quickly, we can start with a &lsquo;pre-trained&rsquo; model that has been developed by experts and trained on large databases. rather than building a model from scratch, we can stand on the shoulders of giants and modify a pre-trained model for this purpose.</p><p>the crux of this project boils down to this step: modifying (or <em>fine tuning</em>) a pre-trained model. the starting model is called resnet18, which was trained on more than a million images. it&rsquo;s very efficient at image classification and so is perfect to be fine-tuned. because resnet18 is already able to recognise objects, the fune-tuning step requires us to provide specific data on the thing we want to recognise. in this case, we&rsquo;ll need to provide images of hand gestures and the corresponding &rsquo;label&rsquo;, i.e. what hand gesture is being done in a photo.</p><a href=#the-search-for-data><h1 id=the-search-for-data><span class=hanchor arialabel=Anchor># </span>the search for data</h1></a><p>so the search for data began. i tried DuckDuckGo, Google, and even Bing! i tried many combinations of search terms, such as <code>hand thumb down -angry</code> but to no avail. i considered supplying the model with my own photos, but realised that the amount i would need to do would be impractical. then, unexpectedly, Hagrid saved me.</p><p><img src=/digital-garden/notes/images/hagrid.jpg width=auto></p><p>i mean,
<a href=https://arxiv.org/abs/2206.08219 rel=noopener>HaGRID - HAnd Gesture Recognition Image Dataset</a>. this is an enormous dataset of ~550k images, each of a person doing one of 18 gestures such as thumbs up, one finger, two fingers, etc. wonderful! just what i was looking for.</p><blockquote class=hint-callout><p>where to find data</p><p>Kaggle has all kinds of cleaned data. You&rsquo;ll find data suitable for similar tasks
<a href="https://www.kaggle.com/datasets?tags=13207-Computer+Vision" rel=noopener>here</a></p></blockquote><p>the full size of this dataset is 716GB, the largest dataset i&rsquo;ve ever worked with. i figured that i wouldn&rsquo;t be able to just use my free tier of Kaggle to handle such a large dataset, so i used the supplied sample dataset which containsed 100 of each of the 18 gestures. training a model on this set was a good first step, but didn&rsquo;t provide the results to be useful.</p><p>next, i found a scaled down version of this dataset, with 512p images instead of the higher quality original photos. these were modified to be lower resolution for image classification instead of object detection (i.e. recognising where in the image a hand is located). i hadn&rsquo;t even realised that the original HaGRID dataset was meant for both purposes. by finding this dataset, i was able to continue with the project.</p><a href=#model><h1 id=model><span class=hanchor arialabel=Anchor># </span>model</h1></a><blockquote><p>after loading the dataset, it was time to get to grips with the fast.ai library. there were many notebooks online from which to draw inspiration and lessons. in particular, there was a notebook which went through the different steps of getting the image files from the kaggle dataset, marking them with a classification label so it could then be loaded into the fast.ai library. i still have lots to learn here, but i accept glossing over this particular section in the name of learning more deep learning.</p></blockquote><p>after loading the dataset, it was time to grapple with the fast.ai library. luckily there are many notebooks online from which to draw inspiration and lessons. in particular
<a href=https://www.kaggle.com/code/stpeteishii/hagrid-18-classify-fasiai/data rel=noopener>this</a> notebook went through the different steps of accessing the files from the kaggle dataset, marking them with a classification label so they can be loaded into the fast.ai library.</p><p>once data is prepared, it needs to be packaged into a &ldquo;data loader&rdquo; object, <code>dls</code> in the code block below. here, the images are already in the <code>train_df</code> dataframe, with <code>fn_col</code> and <code>label_col</code> instructing the program where to look for paths to the image and its label, respectively. the <code>valid_pct</code> parameter is the percentage of items to set aside for verification in each training epoch.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dls</span> <span class=o>=</span> <span class=n>ImageDataLoaders</span><span class=o>.</span><span class=n>from_df</span><span class=p>(</span><span class=n>train_df</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                               <span class=n>fn_col</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=c1>#path</span>
</span></span><span class=line><span class=cl>                               <span class=n>label_col</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=c1>#label</span>
</span></span><span class=line><span class=cl>                               <span class=n>folder</span><span class=o>=</span><span class=s1>&#39;./&#39;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                               <span class=n>valid_pct</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                               <span class=n>item_tfms</span><span class=o>=</span><span class=n>Resize</span><span class=p>(</span><span class=mi>224</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>dls</span><span class=o>.</span><span class=n>show_batch</span><span class=p>(</span><span class=n>max_n</span><span class=o>=</span><span class=mi>9</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><img src=https://www.kaggleusercontent.com/kf/107717335/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Gm8Fz8Ptp1uHi8G9I9iaQQ.K4SK7TDyxs6a4pvyK_B7ATHsGQ8qzuBxaQnK-fA9y4pI8b8ffdPRnV2RBFvaqMEOJxsVjreD-EKHz-5d7FSOBeXtXxKx_T9eiYJzaNHDf0lesuj0qa2MaTs427L__gQ6MGyiBnQ34HCfsXD71m4v2KDAa-l-S_OnB4zoFRwakD80162t8OlYQKNtmriVzjX1ThJmumkc3LHOtfnqmmRwp3oq1P-TcICjsOLVRRuwd0qcIJmcs_sVxLWME3Mu556OdwkAqL9G2oO0DULip2X2cKffhauxA4jChxPIvRbj6RQYpM_cC9krZu_SyzgG9lXfy_BU9Jq7mTQ94gbKVSBgURBoRxm8qjiDtbqHqazkZ34DiMZ6gHie1WZB1KTS9jSdzmjMs0KShX3jEaIEsYOuhoxHrLjllALZiNGkr6CdUj4xBNKxQosMIFdBdGiljQK7HwF4kXFjp4xa63qCmWn0ViB29GM3aJix79SSK17JktpArENhqjhmlhB_LZ1fN33Df8wJkDPOIeO-LwltO8Vfia593oP8oFUAaZ_gyU58NR4sQNFPp7IA447FH__nO1mqVZ4SbW2T7CKjkur025eMmRPI8RLJOFssGj15M64syV8k73TFPg6NBs-zkpzm7B40brm9QaGRcPmCFnCDsQ8hdGnYKiQuL7IUk0HVKqPXyvI.Jux3vjQ24F75ZM-NquPT_A/__results___files/__results___11_0.png width=auto alt></p><p>each of the nine images above are now the same size, and have been labelled appropriately. after the images are loaded in and labelled, the next step is to pass them into the deep learner. here is where we specify the pre-trained model (resnet18) that we&rsquo;d like to begin with.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>learn</span> <span class=o>=</span> <span class=n>vision_learner</span><span class=p>(</span><span class=n>dls</span><span class=p>,</span> <span class=n>resnet18</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=n>error_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>learn</span><span class=o>.</span><span class=n>fine_tune</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><img src=/digital-garden/notes/images/fine_tuning.png width=auto></p><p>after twelve cycles, the model correctly classifies hand gestures 70% of the time. any further fine tuning did not make this any more accurate. i have some ideas as to why that might be, which i will investigate in the future.</p><p>the model has been deployed to a site called HuggingFace
<a href=https://huggingface.co/spaces/harleygray/fastai-hand-classifier rel=noopener>here</a>. you can upload your own photo of you doing a hand gesture and see if it recognises which you are doing!</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digital-garden// data-ctx="ai can tell what hand gesture you're making" data-src=/ class=internal-link>üì° hi, welcome</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://harleygray.github.io/digital-garden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Harley Gray using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=https://harleygray.github.io/digital-garden/>Home</a></li><li><a href=https://twitter.com/harleyraygray>Twitter</a></li><li><a href=https://github.com/harleygray>Github</a></li></ul></footer></div></div></body></html>